{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a RAG Chat Bot the Hard Way with Google Vertex AI, BigQuery & LangChain\n",
        "## Overview\n",
        "In this lab you will build a chat bot that uses documentation to inform its answers. The Public LangChain Documentation will be used in this example. You will also keep track of the conversation history so that we can ask follow up questions to the chat bot. Finally, you'll implement a \"Reset Chat\" function to clear this chat history so that you can \"Change the subject\".\n",
        "\n",
        "## Objectives\n",
        "In this tutorial you will learn how to:\n",
        " * Load and Chunk Documents using LangChain\n",
        " * Create vector embeddings from document chunks using Vertex AI and LangChain\n",
        " * Use BigQuery as a LangChain Vector Store\n",
        " * Use LangChain to build multiple Chains\n",
        "   * History Aware Chain\n",
        "   * Retrieval Question and Answer Chain\n",
        " * Manage Message History using SQL\n",
        " * Return Answers to Questions including the entire message history."
      ],
      "metadata": {
        "id": "gYrzI43n8Lmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and requirements"
      ],
      "metadata": {
        "id": "qW3GrZWFxK9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Your GCP Project\n",
        "In order to get started you'll need to authenticate to your GCP Project."
      ],
      "metadata": {
        "id": "gnuTaY9KUm6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "\n",
        "\n",
        "PROJECT_ID = 'cody-hill-project-293913' # @param {type:\"string\"}\n",
        "\n",
        "auth.authenticate_user(project_id=PROJECT_ID)\n",
        "\n",
        "!gcloud config set project $PROJECT_ID"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIsQQapJUx4D",
        "outputId": "4051b95c-9870-408b-9fd6-d4743d395243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Packages\n",
        "The following Python Packages will be needed to implement all of these features.\n",
        "\n",
        "Once this step is done you'll be prompted to \"restart the session\". Go ahead and do that."
      ],
      "metadata": {
        "id": "z49opeKl40Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-google-vertexai langchain-google-community[featurestore] google-cloud-storage google-cloud-bigquery"
      ],
      "metadata": {
        "id": "TOTrgk3q3EM9",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8ccd6c0b-1850-452e-b78d-8e2110ec8f45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.15-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-google-vertexai\n",
            "  Downloading langchain_google_vertexai-1.0.10-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.10/dist-packages (3.25.0)\n",
            "Collecting langchain-google-community[featurestore]\n",
            "  Downloading langchain_google_community-1.0.8-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.35 (from langchain)\n",
            "  Downloading langchain_core-0.2.35-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.106-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.56.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-vertexai) (1.63.0)\n",
            "Collecting google-cloud-storage\n",
            "  Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from langchain-google-vertexai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-google-vertexai)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.17.1 in /usr/local/lib/python3.10/dist-packages (from langchain-google-community[featurestore]) (2.19.1)\n",
            "Requirement already satisfied: google-api-python-client<3.0.0,>=2.122.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-community[featurestore]) (2.137.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.62.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-community[featurestore]) (1.64.1)\n",
            "Collecting langchain-community<0.3.0,>=0.2.1 (from langchain-google-community[featurestore])\n",
            "  Downloading langchain_community-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.3.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: db-dtypes<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-community[featurestore]) (1.3.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-community[featurestore]) (2.25.0)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-community[featurestore]) (2.1.4)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain-google-community[featurestore]) (14.0.2)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (24.1)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=2.17.1->langchain-google-community[featurestore]) (1.64.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=2.17.1->langchain-google-community[featurestore]) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0,>=2.17.1->langchain-google-community[featurestore]) (1.24.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.48.2)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community[featurestore]) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community[featurestore]) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.122.0->langchain-google-community[featurestore]) (4.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (1.12.5)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (2.0.6)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (0.16)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (3.8)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<0.28.0,>=0.27.0->langchain-google-vertexai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.3.0,>=0.2.1->langchain-google-community[featurestore])\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.35->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.35->langchain) (4.12.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->langchain-google-community[featurestore]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->langchain-google-community[featurestore]) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.1->langchain-google-community[featurestore])\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.1->langchain-google-community[featurestore])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.56.0->langchain-google-vertexai) (0.13.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client<3.0.0,>=2.122.0->langchain-google-community[featurestore]) (3.1.4)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.35->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (1.2.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.1->langchain-google-community[featurestore])\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain-0.2.15-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_vertexai-1.0.10-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_community-0.2.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.35-py3-none-any.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.106-py3-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.6/150.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langchain_google_community-1.0.8-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: tenacity, orjson, mypy-extensions, marshmallow, jsonpointer, httpx-sse, h11, typing-inspect, jsonpatch, httpcore, httpx, dataclasses-json, langsmith, langchain-core, google-cloud-storage, langchain-text-splitters, langchain-google-vertexai, langchain, langchain-community, langchain-google-community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 2.8.0\n",
            "    Uninstalling google-cloud-storage-2.8.0:\n",
            "      Successfully uninstalled google-cloud-storage-2.8.0\n",
            "Successfully installed dataclasses-json-0.6.7 google-cloud-storage-2.18.2 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 httpx-sse-0.4.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.15 langchain-community-0.2.13 langchain-core-0.2.35 langchain-google-community-1.0.8 langchain-google-vertexai-1.0.10 langchain-text-splitters-0.2.2 langsmith-0.1.106 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 tenacity-8.3.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "8ec9c6e558fd4d66b90456e0d379e762"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GCP Services\n",
        "The follwoing GCP Services will need to be enabled"
      ],
      "metadata": {
        "id": "RDdl6g-343-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud services enable aiplatform.googleapis.com bigquery.googleapis.com storage-api.googleapis.com"
      ],
      "metadata": {
        "id": "ghS_VnUG49Sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2180041-3ed8-4885-b7f7-f3e7b0f90ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operation \"operations/acat.p2-915773244463-809ccc63-3aa4-4006-a8b0-7a5d4e6e586d\" finished successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Building the Vector Store\n",
        "In order to be able to use your own documentation and dynamically retrieve the correct pieces of documentation based on the question that is asked, you need to create vector embeddings of all of your documentation. There are multiple steps involved in getting the documentation usable for Retrival Augmented Generation (***RAG***).\n",
        " * Download the Documentation\n",
        " * Load the Documentation\n",
        " * Break the Documentation into smaller \"Searchable\" pieces (***Chunks***)\n",
        " * Create embeddings of each of these chunks\n",
        " * Store these embeddings along with their content into a vector store."
      ],
      "metadata": {
        "id": "bH3EFhunxh6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Setting Variables\n",
        "Below are all of the variables you will need in order to build out the Vector Store. The only variable the *needs* to be changed is `PROJECT_ID` everything else is fine how it is.\n",
        "  * **PROJECT_ID** *(Change Me)*:\n",
        "    * This is your Google Cloud Project ID and is needed to authenticate to different services in Google Cloud\n",
        "  * **REGION**:\n",
        "    * This is the region in Google Cloud that you would like to utilize services within.\n",
        "  * **EMBEDDING_MODEL**:\n",
        "    * This is the text embedding model that you will use to create embeddings from our text chunks. As of writing this Colab. `text-embedding-004` is the latest text embedding model.\n",
        "  * **BQ_DATASET**:\n",
        "    * This is the name of the BiqQuery Dataset that you will be creating our Table to store our Embeddings and Text. (This BQ_DATASET must already exist)\n",
        "  * **BQ_TABLE**:\n",
        "    * This is the name of the BigQuery table that will be used to store our Embeddings and Text. (This will be created automatically by LangChain)\n",
        "  * **CHUNK_SIZE**:\n",
        "    * This is how large you would like each of our text chunks to be (in number of characters). The larger this number the more content that will be in each chunk, but the larger the chunk is, the less unique the results of each semantic search becomes. So striking a balance between good searchability and enough content to inform the LLM is important.\n",
        "  * **CHUNK_OVERLAP**:\n",
        "    * This is how much overlap (in number of characters) you would like in each chunk. In order to try and not lose context, when a document is broken into chunks, it may be in the middle of a paragraph, sentence, or even a word. So to make sure each chunk has enough context, you will be storing some of the text from the previous chunk in the following chunk.\n",
        "  * **DOCS_BUCKET_NAME**:\n",
        "    * This is the GCP Bucket that houses the documentation that you will be downloading, embedding, and storing.\n",
        "      * Don't update this unless you know what you are doing.\n",
        "  * **DOCS_DIR**:\n",
        "    * This is the local directory that you will be downloading the documentation from the GCP Bucket into.\n",
        "\n"
      ],
      "metadata": {
        "id": "33fJOmZry3Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = 'cody-hill-project-293913' # @param {type:\"string\"}\n",
        "REGION = 'us-central1' # @param {type:\"string\"}\n",
        "EMBEDDING_MODEL = 'text-embedding-004' # @param {type:\"string\"}\n",
        "BQ_DATASET = 'doing_it' # @param {type:\"string\"}\n",
        "BQ_TABLE = 'the_hard_way' # @param {type:\"string\"}\n",
        "CHUNK_SIZE = 2000 # @param {type:\"integer\"}\n",
        "CHUNK_OVERLAP = 200 # @param {type:\"integer\"}\n",
        "DOCS_BUCKET_NAME = 'ch-langchain-docs' # @param {type:\"string\"}\n",
        "DOCS_DIR = 'documentation' # @param {type:\"string\"}"
      ],
      "metadata": {
        "id": "aDvGR2cU4A7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Python Imports\n",
        "These are all of the python modules you'll need in order to create our Vector Store. These will be descibed in greater detail as you use them."
      ],
      "metadata": {
        "id": "YQyl_Fst2ydY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI0mIRX_14kF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from time import sleep\n",
        "from google.cloud import storage, bigquery\n",
        "import google.api_core.exceptions\n",
        "from langchain_google_vertexai import VertexAIEmbeddings\n",
        "from langchain_google_community import BigQueryVectorSearch\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Downloading the documentation locally\n",
        "This function will download all of the content from the GCP Bucket `DOCS_BUCKET_NAME` to the local directory `DOCS_DIR` and print out how many files were downloaded."
      ],
      "metadata": {
        "id": "1sjMmZZL3Nm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = storage.Client()\n",
        "source_bucket = client.bucket(DOCS_BUCKET_NAME)\n",
        "blobs = source_bucket.list_blobs()\n",
        "downloads = 0\n",
        "for blob in blobs:\n",
        "    object_path = blob.name\n",
        "    local_directory = os.path.join(DOCS_DIR, os.path.dirname(object_path))\n",
        "    os.makedirs(local_directory, exist_ok=True)\n",
        "    local_path = os.path.join(local_directory, os.path.basename(object_path))\n",
        "    blob.download_to_filename(local_path)\n",
        "    downloads += 1\n",
        "print(f\"Number of downloads:  {downloads}\")"
      ],
      "metadata": {
        "id": "1CqudyXR6CcS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6098284-14cb-4961-b618-913363f0f5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of downloads:  1350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4: Loading Documents\n",
        "This function loads all of the files from the `DOCS_DIR` that match the `glob` of `**/*.md*`. This basically means search this directory recursively and load any file as a LangChain \"Document\" object that has a file extension of `.md` or `.mdx`. Then print how many documents were \"Loaded\""
      ],
      "metadata": {
        "id": "ixWKERCP4Rlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_loader_kwargs = {'autodetect_encoding': True}\n",
        "loader = DirectoryLoader(DOCS_DIR,\n",
        "                          glob=\"**/*.md*\",\n",
        "                          use_multithreading=True,\n",
        "                          loader_cls=TextLoader,\n",
        "                          loader_kwargs=text_loader_kwargs)\n",
        "docs = loader.load()\n",
        "print(f\"Number of Docs:  {len(docs)}\")"
      ],
      "metadata": {
        "id": "FNCoRsvM88xW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a1d579f-8808-461f-bfc4-6143d221b97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Docs:  1346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5: Chunking Documents\n",
        "This function will take each document and break it into \"Chunks\" based on our `CHUNK_SIZE`, `CHUNK_OVERLAP`, & `separators`.\n",
        "\n",
        "The `CHUNK_SIZE` & `CHUNK_OVERLAP` were described in the `Setting Variables` section above, but let's talk about the `separators`.\n",
        "\n",
        "Separators are what you would like your document \"Split\" by. In the example below:\n",
        "```python\n",
        "[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
        "```\n",
        "You are telling the splitter to try and find a place to split the document around the `CHUNK_SIZE` that ends with these characters in order of priority:\n",
        "  * `\"\\n\\n\"`: Double Line break normally indicates the end of a section\n",
        "  * `\"\\n\"`: Single line break normally indicates the end of a paragraph.\n",
        "  * `\".\"`, `\"!\"`, & `\"?\"`: A period, exclamation mark, or question mark normally indicates the end of a sentence.\n",
        "  * `\",\"`: Splitting at a comma is better than splitting between random words.\n",
        "  * `\" \"`: Splitting at a space is better than splitting in the middle of a word.\n",
        "  * `\"\"`: Finally if no other option is available, then split the document at any character."
      ],
      "metadata": {
        "id": "igcbWPt95Tz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"]\n",
        ")\n",
        "chunked_docs = []\n",
        "for doc in docs:\n",
        "    chunks = text_splitter.split_documents([doc])\n",
        "    for idx, split in enumerate(chunks):\n",
        "        split.metadata[\"chunk\"] = idx\n",
        "    chunked_docs.append(chunks)\n",
        "print(f\"Number of Document Chunks:  {len(chunked_docs)}\")"
      ],
      "metadata": {
        "id": "2GtBaRhDnRYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "540c7ef0-e2ef-44a7-8621-c11ceca52e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Document Chunks:  1346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6a: Embedding and Storing the Documents\n",
        "#### **!!!Only run this if you have over 1 Hour to Create the Vector Store If you do not, skip this step and run Task 7b instead!!!**\n",
        "In this step you are defining our embedding model as well as our vector store, and allowing LangChain to embed and store our documents into BigQuery.\n",
        "\n",
        "In this case you are using the `EMBEDDING_MODEL` variable to define which model we'd like to use in our `embedding_engine`.\n",
        "\n",
        "You are then providing the `embedding_engine` to the `BigQueryVectorSearch` along with the `BQ_DATASET` & `BQ_TABLE` that you'd like our embeddings stored in.\n",
        "\n",
        "You are then looping through each document and using `store.add_documents(doc)` to save our documentation content along with the embeddings into our BigQuery Table.\n",
        "\n",
        "The rest of the code is to handle \"Slowing things down\" in the case that you exceed our quota or rate limit when creating these embeddings or saving them to BigQuery.\n",
        "\n",
        "#### **NOTE:**\n",
        "To run this, you'll need to edit the code in the try block and uncomment `store.add_documents(doc)` and comment out `pass`"
      ],
      "metadata": {
        "id": "5ikBIizqDVrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_engine = VertexAIEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL,\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION,\n",
        "    request_parallelism=250,\n",
        "    max_retries=10\n",
        ")\n",
        "store = BigQueryVectorSearch(\n",
        "    project_id=PROJECT_ID,\n",
        "    dataset_name=BQ_DATASET,\n",
        "    table_name=BQ_TABLE,\n",
        "    location=REGION,\n",
        "    embedding=embedding_engine\n",
        ")\n",
        "\n",
        "client = bigquery.Client()\n",
        "dataset_ref = client.dataset(BQ_DATASET)\n",
        "dataset = bigquery.Dataset(dataset_ref)\n",
        "dataset.location = REGION\n",
        "client.create_dataset(dataset, exists_ok=True)\n",
        "\n",
        "max_retries = 10\n",
        "initial_delay = 1\n",
        "max_delay = 30\n",
        "for doc in chunked_docs:\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # store.add_documents(doc)\n",
        "            # uncomment the line above if you actually would like to create the\n",
        "            # BigQuery Vector Store\n",
        "            pass\n",
        "            break\n",
        "        except google.api_core.exceptions.Forbidden as e:\n",
        "            if \"Exceeded rate limits\" in str(e) or \"quotaExceeded\" in str(e):\n",
        "                delay = min(initial_delay * 2**attempt, max_delay)\n",
        "                delay += random.uniform(0, delay)\n",
        "                print(f\"Retrying in {min(delay, max_delay)} seconds...\")\n",
        "                sleep(min(delay, max_delay))\n",
        "            else:\n",
        "                print(f\"Error: {e}\")\n",
        "                raise\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            raise\n",
        "        if attempt - 1 == max_retries:\n",
        "            print(\"Hit maximum retries, giving up...\")"
      ],
      "metadata": {
        "id": "OBZLAWW-nupB",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "1d8704aa-d197-4f43-c678-be39e759107e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFound",
          "evalue": "404 POST https://bigquery.googleapis.com/bigquery/v2/projects/cody-hill-project-293913/queries?prettyPrint=false: Not found: Dataset cody-hill-project-293913:doing_it was not found in location us-central1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2a92e6dea89b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmax_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m store = BigQueryVectorSearch(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mproject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPROJECT_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBQ_DATASET\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m                         \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 obj.__init__ = functools.wraps(obj.__init__)(  # type: ignore[misc]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_community/bigquery_vector_search.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embedding, project_id, dataset_name, table_name, location, content_field, metadata_field, text_embedding_field, doc_id_field, distance_strategy, credentials)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_index_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_vector_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_initialize_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_google_community/bigquery_vector_search.py\u001b[0m in \u001b[0;36m_initialize_vector_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;34mf\" table_name = '{self.table_name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             )\n\u001b[0;32m--> 228\u001b[0;31m             job = self.bq_client.query(\n\u001b[0m\u001b[1;32m    229\u001b[0m                 \u001b[0mcheck_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbigquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueryApiMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUERY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, api_method)\u001b[0m\n\u001b[1;32m   3479\u001b[0m         \u001b[0;31m# _default_query_job_config) up to this point.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3480\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mapi_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0menums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQueryApiMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQUERY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3481\u001b[0;31m             return _job_helpers.query_jobs_query(\n\u001b[0m\u001b[1;32m   3482\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3483\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/_job_helpers.py\u001b[0m in \u001b[0;36mquery_jobs_query\u001b[0;34m(client, query, job_config, location, project, retry, timeout, job_retry)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_to_query_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;31m# The future might be in a failed state now, but if it's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/_job_helpers.py\u001b[0m in \u001b[0;36mdo_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mrequest_body\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requestId\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_job_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0mspan_attributes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         api_response = client._call_api(\n\u001b[0m\u001b[1;32m    303\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mspan_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BigQuery.query\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36m_call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    831\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattributes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspan_attributes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             ):\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             _retry_error_helper(\n\u001b[0m\u001b[1;32m    154\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         )\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/_http/__init__.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexpect_json\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFound\u001b[0m: 404 POST https://bigquery.googleapis.com/bigquery/v2/projects/cody-hill-project-293913/queries?prettyPrint=false: Not found: Dataset cody-hill-project-293913:doing_it was not found in location us-central1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6b: Import Vector Store to BigQuery\n",
        "Because it takes so long to create the Vector Embeddings and push them to BigQuery, this step will allow you to simply import the vector store instead of having to create it from scratch."
      ],
      "metadata": {
        "id": "FwXkgksKwWis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#client = bigquery.Client()\n",
        "client = bigquery.Client(PROJECT_ID)\n",
        "\n",
        "dataset_ref = client.dataset(BQ_DATASET)\n",
        "dataset = bigquery.Dataset(dataset_ref)\n",
        "dataset.location = REGION\n",
        "client.create_dataset(dataset, exists_ok=True)\n",
        "\n",
        "avro_file_path = f'{DOCS_DIR}/bq_the_hard_way.avro'\n",
        "\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.AVRO\n",
        ")\n",
        "\n",
        "with open(avro_file_path, 'rb') as source_file:\n",
        "    job = client.load_table_from_file(\n",
        "        source_file,\n",
        "        f'{BQ_DATASET}.{BQ_TABLE}',\n",
        "        job_config=job_config,\n",
        "    )\n",
        "\n",
        "job.result()\n",
        "\n",
        "print(f'Loaded {job.output_rows} rows into {BQ_DATASET}.{BQ_TABLE}')"
      ],
      "metadata": {
        "id": "hOMpLWJJySN9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a10b25-159c-4d6e-bf4a-ff3b597c964f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 13553 rows into doing_it.the_hard_way_denis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Building the Chat Bot\n",
        "Now that you have all of the content that you would like our chatbot to use as stored in BigQuery, you need to build out the front end of the chat bot.\n",
        "This consists of the following:\n",
        "  * Fetching message history if this isn't the first message in the chat.\n",
        "  * Embedding the users message\n",
        "  * Using these embeddings to search for the correct documentation that is most similar to the user's message\n",
        "  * Provide the chat history, retreived documentation, and the user's message to the LLM for a response.\n",
        "  * Storing the conversation history.\n",
        "  * Displaying the conversation to the user\n",
        "\n"
      ],
      "metadata": {
        "id": "JMciyOphpxDw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Setting Variables\n",
        "Below are all of the variables you will need in order to build out the Vector Store. The only varibale the *needs* to be changed is `PROJECT_ID` everything else can remain the same.\n",
        "  * **PROJECT_ID** *(Change Me)*:\n",
        "    * This is your Google Cloud Project ID and is needed to authenticate to different services in Google Cloud\n",
        "  * **REGION**:\n",
        "    * This is the region in Google Cloud that you would like to utilize services within.\n",
        "  * **LLM_MODEL**:\n",
        "    * This is the Large Language Model that you will use to respond to the users messages. As of writing this Colab, `gemini-1.5-pro-001` is our most capable model.\n",
        "  * **EMBEDDING_MODEL**:\n",
        "    * This is the text embedding model that you will use to create embeddings from our text chunks. As of writing this Colab. `text-embedding-004` is the latest text embedding model.\n",
        "  * **BQ_DATASET**:\n",
        "    * This is the name of the BiqQuery Dataset that you will be creating our Table to store our Embeddings and Text. (This BQ_DATASET must already exist)\n",
        "  * **BQ_TABLE**:\n",
        "    * This is the name of the BigQuery table that will be used to store our Embeddings and Text. (This will be created automatically by LangChain)\n",
        "  * **MAX_OUTPUT_TOKEN**:\n",
        "    * This parameter will tell the Large Language Model the maximum amount of tokens it is allowed in it's response.\n",
        "  * **TEMPERATURE**:\n",
        "    * This is the model's temperature. The temperature determins how creative or factual the model is. Where a temperature of 0.0 is not creative at all and will result in much more factual responses, and a temperature of 1.0 is very creative and will result in non-fact based answers.\n",
        "  * **SQL_CONNECTION_STRING**:\n",
        "    * Here you are defining a `SQLAlchemy` compatible connection string to be used for message chat memory."
      ],
      "metadata": {
        "id": "EA-ZL2OrORnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ID = 'cody-hill-project-293913' # @param {type:\"string\"}\n",
        "REGION = 'us-central1' # @param {type:\"string\"}\n",
        "LLM_MODEL = 'gemini-1.5-pro-001' # @param {type:\"string\"}\n",
        "EMBEDDING_MODEL = 'text-embedding-004' # @param {type:\"string\"}\n",
        "BQ_DATASET = 'doing_it' # @param {type:\"string\"}\n",
        "BQ_TABLE = 'the_hard_way_denis' # @param {type:\"string\"}\n",
        "MAX_OUTPUT_TOKENS = 8192 # @param {type:\"integer\"}\n",
        "TEMPERATURE = 0.1 # @param\n",
        "SQL_CONNECTION_STRING = 'sqlite:///sqlite.db' # @param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "i4ZqRHKoq6qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Python Imports\n",
        "These are all of the Python modules you'll need in order to create our Vector Store. These will be describe in greater detail as you use them."
      ],
      "metadata": {
        "id": "m4McvedcQHe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings, HarmBlockThreshold, HarmCategory\n",
        "from langchain_google_community import BigQueryVectorSearch\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains import create_history_aware_retriever\n",
        "import langchain"
      ],
      "metadata": {
        "id": "18t9mxmHp-dB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Define Embedding Model and Vector Store\n",
        "Just like when you created the Vector Store, you need to define how you want to create our embeddings and where our embeddings are stored. The difference this time around is that instead of storing the embeddings, you'll be using these embeddings to search for simliar content."
      ],
      "metadata": {
        "id": "KfY2sB0KUcZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_retriever():\n",
        "    embedding_engine = VertexAIEmbeddings(\n",
        "          model_name=EMBEDDING_MODEL,\n",
        "          project=PROJECT_ID,\n",
        "          location=REGION,\n",
        "          request_parallelism=250\n",
        "    )\n",
        "    store = BigQueryVectorSearch(\n",
        "        project_id=PROJECT_ID,\n",
        "        dataset_name=BQ_DATASET,\n",
        "        table_name=BQ_TABLE,\n",
        "        embedding=embedding_engine\n",
        "    )\n",
        "    retriever = store.as_retriever(search_kwargs={\"k\": 10})\n",
        "    return retriever\n",
        "\n",
        "#get_retriever()"
      ],
      "metadata": {
        "id": "kZ8PNiviQX7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4: Define Large Language Model\n",
        "Here you are defining the large language model you want to use to respond to the user's message. You are setting our `safety_settings` in each category to their lowest setting. This isn't recommended for production, but it shows an example of how you can modify these settings.\n",
        "\n",
        "You are then using the variables `LLM_MODEL`, `MAX_OUTPUT_TOKENS`, & `TEMPERATURE`. Along with the `safety_settings` to define the Large Language Model"
      ],
      "metadata": {
        "id": "Nshsc0F0VPnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm():\n",
        "    safety_settings = {\n",
        "        HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    }\n",
        "\n",
        "    llm_model = ChatVertexAI(\n",
        "        model_name=LLM_MODEL,\n",
        "        safety_settings=safety_settings,\n",
        "        max_tokens=MAX_OUTPUT_TOKENS,\n",
        "        temperature=TEMPERATURE\n",
        "    )\n",
        "    return llm_model"
      ],
      "metadata": {
        "id": "ccmHDjs-QvLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5: Define History Aware Chain\n",
        "Now you're starting to utilize the \"chaining\" in LangChain.\n",
        "\n",
        "In this step you are creating a \"chain\" to take into account the previous conversation that may have taken place, along with the latest question. Both of these will be used together along with instructions to create a \"History Aware Retriever\" to be used to answer the question in a later step."
      ],
      "metadata": {
        "id": "0bUHJLVxWFE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_history_aware_chain(llm_model, retriever):\n",
        "    contextualize_q_system_prompt = (\n",
        "        \"Given a chat history and the latest user question which might reference context in the chat history, \"\n",
        "        \"formulate a standalone question which can be understood without the chat history. Do NOT answer the question, \"\n",
        "        \"just reformulate it if needed and otherwise return it as is.\"\n",
        "    )\n",
        "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", contextualize_q_system_prompt),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "    history_aware_retriever = create_history_aware_retriever(\n",
        "        llm_model, retriever, contextualize_q_prompt\n",
        "    )\n",
        "\n",
        "    return history_aware_retriever"
      ],
      "metadata": {
        "id": "O8HwL4_XRUBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6: Define Question and Answer Chain\n",
        "In this step you're creating a chain that will answer the user's question or follow up question, based on the conversation history."
      ],
      "metadata": {
        "id": "kr1z87mUW-9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_question_answer_chain(llm_model):\n",
        "  qa_system_prompt = (\n",
        "    \"Instructions:  You are a knowledgeable LangChain assistant that answers questions. \"\n",
        "    \"Using the following pieces of documentation, explain in great detail how to answer the Human's question. \"\n",
        "    \"Provide Code examples with explanations whenever possible.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        "  )\n",
        "  qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  question_answer_chain = create_stuff_documents_chain(llm_model, qa_prompt)\n",
        "  return question_answer_chain, qa_prompt"
      ],
      "metadata": {
        "id": "2jegHwcqR1wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ufUlU-VoCwfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 7: Define \"Full Chain\"\n",
        "In this step you are combining the both the History Aware Chain with the Question and Answer Chain to create a full \"Conversational, RAG, QA Chain!\"\n",
        "\n",
        "You're passing in the `rag_chain` as well as all of the message history, to create the final \"Runnable\" LangChain object that will call to the LLM and respond to the user's messages.\n",
        "\n",
        "The conversation history is stored by using a unique `session_id`. In this implementation you're using the [SQLChatMessageHistory](https://python.langchain.com/v0.1/docs/integrations/memory/sql_chat_message_history/). This can use any `SQLAlchemy` compatible `connection_string`. In this example we're using `sqlite` but there are many different [memory integrations available in LangChain](https://python.langchain.com/v0.1/docs/integrations/memory/)."
      ],
      "metadata": {
        "id": "9mJ4Smf1X62_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_full_chain(history_aware_chain, question_answer_chain, session_id):\n",
        "    rag_chain = create_retrieval_chain(history_aware_chain, question_answer_chain)\n",
        "\n",
        "    conversational_rag_chain = RunnableWithMessageHistory(\n",
        "        rag_chain,\n",
        "        lambda session_id: SQLChatMessageHistory(\n",
        "            session_id=session_id, connection_string=SQL_CONNECTION_STRING\n",
        "        ),\n",
        "        input_messages_key=\"input\",\n",
        "        history_messages_key=\"chat_history\",\n",
        "        output_messages_key=\"answer\",\n",
        "    )\n",
        "    return conversational_rag_chain"
      ],
      "metadata": {
        "id": "64VT51IdTSoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 8: Define Question Answering\n",
        "You're finally ready to put all of this together and answer the users question.\n",
        "\n",
        "Here you are calling all of the functions that you have previously defined to fetch the correct documentation, taking into account the conversation history, and answering the question with that context."
      ],
      "metadata": {
        "id": "SUcrZNJYZKYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question, session_id):\n",
        "    retriever = get_retriever()\n",
        "    llm_model = get_llm()\n",
        "    history_aware_chain = create_history_aware_chain(llm_model, retriever)\n",
        "    question_answer_chain, qa_prompt = create_question_answer_chain(llm_model)\n",
        "    conversational_rag_chain = create_full_chain(history_aware_chain, question_answer_chain, session_id)\n",
        "\n",
        "    conversational_rag_chain.invoke(\n",
        "        {\"input\": question},\n",
        "        config={\"configurable\": {\"session_id\": session_id}}\n",
        "    )[\"answer\"]"
      ],
      "metadata": {
        "id": "o0BcYdLwqrO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 9: Ask Questions and Get Answers\n",
        "Now you can finally use this application by asking questions and we'll get our entire conversation back including the answer to our latest question.\n",
        "\n",
        "\n",
        "The `question` parameter is pretty self explanatory and allows you to update the message you'll send to this chat bot.\n",
        "\n",
        "The `session_id` parameter is used to store the unique chat conversation. If you were to ask a question with the `session_id` besing set to `conversation123` any follow question using that same `session_id` will be store with `conversation123` if you were to change the `session_id` to `conversation456` you can create a brand new chat with no chat history. You could then change your `session_id` back to `conversation123` and pickup that conversation where you left off.\n",
        "\n",
        "If you would like to simply `clear` you chat history, there is a function below for this.\n",
        "\n",
        "After defining your `question` and `session_id` you'll pass those both to `answer_question` which will automatically add your latest question and it's respone to the `SQLChatMessageHistory`.\n",
        "\n",
        "You can then fetch the latest `chat_history` and iterate through each message. If the message is of type `human` you'll print `Human:` before the message. If not, you'll print `AI:` before the message."
      ],
      "metadata": {
        "id": "0G0w0zGpaA2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"how can i add \\\"context\\\" key to include the sources of my retrieved docs\" # @param {type:\"string\"}\n",
        "session_id = 'MyUniqueChatSession' # @param {type:\"string\"}\n",
        "\n",
        "answer_question(question, session_id)\n",
        "\n",
        "chat_history = SQLChatMessageHistory(\n",
        "    session_id=session_id, connection_string=SQL_CONNECTION_STRING\n",
        ")\n",
        "\n",
        "for message in chat_history.messages:\n",
        "    if message.type == 'human':\n",
        "        print(f\"Human:\\n  {message.content}\\n\\n\")\n",
        "    else:\n",
        "        print(f\"AI:\\n  {message.content}\\n\\n\")\n"
      ],
      "metadata": {
        "id": "lqa0FJ7Zozkm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "81155e35-0fc8-42ec-d526-279efdef974a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "string indices must be integers",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-3266ddbdeb3a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'MyUniqueChatSession'\u001b[0m \u001b[0;31m# @param {type:\"string\"}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0manswer_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m chat_history = SQLChatMessageHistory(\n",
            "\u001b[0;32m<ipython-input-54-9ad1ffa37f14>\u001b[0m in \u001b[0;36manswer_question\u001b[0;34m(question, session_id)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mconversational_rag_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_full_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory_aware_chain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_answer_chain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     conversational_rag_chain.invoke(\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;34m{\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"configurable\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"session_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msession_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 10: Clear Message History\n",
        "This little function simply deletes the conversation history for a given `session_id`"
      ],
      "metadata": {
        "id": "TKNZAIYvfE4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session_id = 'MyUniqueChatSession' # @param {type:\"string\"}\n",
        "\n",
        "chat_history = SQLChatMessageHistory(\n",
        "    session_id=session_id, connection_string=SQL_CONNECTION_STRING\n",
        ")\n",
        "chat_history.clear()"
      ],
      "metadata": {
        "id": "cun91IMtvpQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Congratulations!\n",
        "You have now completed the lab! In this lab, you utilized BigQuery as a Vector Store by: downloading, chucking, embedding, and storing documentation. You then used these embeddings in a RAG architecture to inform a language model's responses. You have also implemented chat history using LangChains \"Memory\" to allow for follow up questions."
      ],
      "metadata": {
        "id": "V_K1HDfJIdiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next Steps\n",
        "* Check out the [Generative AI on Vertex AI documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview).\n",
        "* Check out [LangChain's documentation](https://python.langchain.com/v0.2/docs/introduction/)\n",
        "* Learn more about Generative AI on the [Google Cloud Tech YouTube channel](https://www.youtube.com/@googlecloudtech/)."
      ],
      "metadata": {
        "id": "i8CqwY1OKLGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessment Suggestions for Lab Building Team\n",
        "### Part 1: Task 6a-b\n",
        "The only thing that's really measurable is to see if the user has a BigQuery Dataset and Table named:\n",
        "\n",
        "DATASET = 'doing_it'\n",
        "\n",
        "TABLE = 'the_hard_way'\n",
        "\n",
        "### Part2: Task 9\n",
        "Other than that, if you can check the user's for API utilization for Vertex AI?\n",
        "\n",
        "Maybe if they have any API or quota usage we can check that."
      ],
      "metadata": {
        "id": "ymGbcEEbNCGd"
      }
    }
  ]
}